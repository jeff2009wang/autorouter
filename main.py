import os
import gc
import re
import json
import logging
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Tuple
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ– PyTorch æ˜¾å­˜åˆ†é… [1]
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
from fastapi import FastAPI
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import litellm

# --- ä¾èµ–åº“ ---
from modelscope import snapshot_download
from transformers import AutoModelForVision2Seq, AutoProcessor
from qwen_vl_utils import process_vision_info
from sentence_transformers import CrossEncoder

# ===================== é…ç½®ä¸­å¿ƒ =====================
# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] NekoBrain: %(message)s")
logger = logging.getLogger("NekoBrain")

class Config:
    # ã€æ ¸å¿ƒé…ç½®ã€‘æœ¬åœ°è§†è§‰æ¨¡å‹ (ä»…ç”¨äºå¹•åOCRï¼Œä¸ç›´æ¥å¯¹è¯)
    LOCAL_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

    # èšåˆAPIé…ç½® (å»ºè®®æ”¹ä¸ºç¯å¢ƒå˜é‡)
    AGGREGATOR_API_KEY = "sk-DuctN11czck6s758299ZoeipAjKmlhXcfhGchCZwQttQqI1o"
    AGGREGATOR_BASE_URL = "http://192.168.50.165:3000/v1"

    # æ¨¡å‹æ˜ å°„è¡¨ï¼šå°†è·¯ç”±æ ‡ç­¾æ˜ å°„åˆ°å®é™…åç«¯æ¨¡å‹ [1]
    MODEL_MAP = {
        "general_text": "gemini-3-flash-preview", 
        "logic_king": "gemini-3-pro-preview",
        "deepthink": "gemini-3-pro-deepthink",
        "vibes_master": "MiniMaxAI/MiniMax-M2", 
        "searching": "gemini-3-flash-preview",
        "gpt-5.1": "gpt-5.1" 
    }

    # è¯­ä¹‰è·¯ç”±æè¿°
    ROUTING_DESCRIPTIONS = {
        "general_text": "General conversation, simple greetings, short questions, long essays, summarization, translation, general knowledge.",
        "logic_king": "Programming code, json, debugging, python, algorithms, variable definitions.",
        "deepthink": "Math proofs, complex physics, latex formulas, calculus, step-by-step reasoning.",
        "vibes_master": "Creative writing, roleplay, emotional support, poetry.",
        "searching": "News, current events, real-time weather, fact check."
    }

# ===================== è·¯ç”±å¤§è„‘ (NekoBrain) =====================
class NekoBrain:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.executor = ThreadPoolExecutor(max_workers=1)
        self.full_labels = list(Config.ROUTING_DESCRIPTIONS.keys())
        
        logger.info(f"ğŸ“¸ Initializing NekoBrain with VLM: {Config.LOCAL_MODEL_ID}...")
        self._init_local_models()
        self._init_router_model()

        # ç”Ÿæˆå‚æ•°é…ç½®
        self.generation_configs = {
            "vision": {
                "do_sample": True, "top_p": 0.8, "top_k": 20, "temperature": 0.7,
                "repetition_penalty": 1.0, "max_new_tokens": 1024,
            }
        }

    def _init_local_models(self):
        """åˆå§‹åŒ–æœ¬åœ°è§†è§‰æ¨¡å‹ (Qwen)"""
        try:
            model_dir = snapshot_download(Config.LOCAL_MODEL_ID)
        except Exception:
            model_dir = Config.LOCAL_MODEL_ID 

        self.vlm_model = AutoModelForVision2Seq.from_pretrained(
            model_dir, torch_dtype="auto", device_map="auto", trust_remote_code=True 
        )
        self.processor = AutoProcessor.from_pretrained(model_dir, trust_remote_code=True)
        
        # é™åˆ¶åˆ†è¾¨ç‡ä»¥é˜²æ˜¾å­˜æº¢å‡º [1]
        if hasattr(self.processor, "image_processor"):
            self.processor.image_processor.min_pixels = 256 * 256
            self.processor.image_processor.max_pixels = 1024 * 1024

    def _init_router_model(self):
        """åˆå§‹åŒ–è¯­ä¹‰è·¯ç”±æ¨¡å‹"""
        self.ce_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=self.device)

    def inject_assistant_prompt(self, messages: List[Dict]) -> List[Dict]:
        """æ³¨å…¥ç³»ç»Ÿçº§æç¤ºï¼Œè§„èŒƒ LaTeX æ ¼å¼å’Œæ®µè½"""
        new_msgs = [m.copy() for m in messages]
        injection = {
            "role": "assistant",
            "content": "å¥½çš„ï¼Œæˆ‘ä¼šä¸¥æ ¼æ‰§è¡Œæ ¼å¼è¦æ±‚ï¼šæ•°å­¦å…¬å¼å‰ååŠ ç©ºæ ¼å¹¶ä½¿ç”¨ LaTeXï¼Œä¿æŒæ®µè½æ¸…æ™°ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„å›ç­”ï¼š\n"
        }
        new_msgs.append(injection)
        return new_msgs

    @torch.no_grad()
    def _local_vlm_inference(self, messages: List[Dict], prompt_text: str, mode: str = "vision") -> str:
        """æ‰§è¡Œæœ¬åœ°è§†è§‰æ¨¡å‹æ¨ç†"""
        try:
            qwen_messages = []
            for m in messages:
                if m["role"] == "system": continue 
                
                # æ ¼å¼æ¸…æ´—
                new_m = m.copy()
                if isinstance(new_m.get("content"), list):
                    clean_content = []
                    for item in new_m["content"]:
                        if item.get("type") == "image_url":
                            img_obj = item.get("image_url")
                            url_str = img_obj.get("url") if isinstance(img_obj, dict) else str(img_obj)
                            clean_content.append({"type": "image", "image": url_str})
                        elif item.get("type") == "image":
                            clean_content.append(item)
                        else:
                            clean_content.append(item)
                    new_m["content"] = clean_content
                qwen_messages.append(new_m)
            
            qwen_messages.append({"role": "user", "content": prompt_text})

            text_input = self.processor.apply_chat_template(
                qwen_messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(qwen_messages)
            
            inputs = self.processor(
                text=[text_input],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            ).to(self.vlm_model.device)

            gen_config = self.generation_configs.get(mode, self.generation_configs["vision"])
            
            generated_ids = self.vlm_model.generate(**inputs, **gen_config)
            generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            return output_text
            
        except torch.cuda.OutOfMemoryError:
            logger.error("ğŸ§± CUDA OOM during inference! Clearing cache...")
            torch.cuda.empty_cache()
            return "GENERAL_SCENE" 
        except Exception as e:
            logger.error(f"Inference error: {e}")
            return ""
        finally:
            torch.cuda.empty_cache()
            gc.collect()

    def _get_fused_decision(self, messages: List[Dict]) -> Tuple[str, List[Dict]]:
        """
        æ ¸å¿ƒå†³ç­–é€»è¾‘ï¼š
        1. è§†è§‰æ£€æµ‹ -> æœ¬åœ°OCR
        2. æ–‡æœ¬æå– -> å…³é”®è¯åŒ¹é…æˆ–è¯­ä¹‰æ‰“åˆ†
        """
        has_image = False
        for m in messages[-2:]:
            if isinstance(m.get("content"), list):
                for item in m["content"]:
                    if item.get("type") in ["image", "image_url"]:
                        has_image = True
                        break

        extracted_text = ""
        modified_messages = messages 

        if has_image:
            logger.info("ğŸ“¸ [Vision Detected] Running local Qwen2.5-VL-3B analysis...")
            
            instruction = (
                "Analyze this image. "
                "If it contains document text, code, math formulas, tables, or error logs, "
                "transcribe all the content exactly (OCR). "
                "If it is a general scenery, photo of a person, or artistic image, output 'GENERAL_SCENE'."
            )
            
            vlm_output = self._local_vlm_inference(messages, instruction, mode="vision")
            
            # è§†è§‰å†…å®¹å›é€€é€»è¾‘ [1]
            if "GENERAL_SCENE" in vlm_output or len(vlm_output.strip()) < 5:
                logger.info("ğŸ” [Scene/Fallback] Routing to GPT-5.1.")
                return "gpt-5.1", messages
            else:
                extracted_text = vlm_output
                clean_log = extracted_text.replace('\n', ' ')[:150]
                logger.info(f"ğŸ“œ [OCR Success] Content: {clean_log}...")
                
                modified_messages = []
                for m in messages:
                    new_m = m.copy()
                    if isinstance(new_m["content"], list):
                        new_content = f"ã€User Uploaded Image Content (Local OCR)ã€‘\n{extracted_text}"
                        new_m["content"] = new_content
                    modified_messages.append(new_m)

        # æå–ç”¨äºè·¯ç”±çš„æ–‡æœ¬
        target_text = extracted_text if extracted_text else ""
        if not target_text:
            last_msg = modified_messages[-1]
            if isinstance(last_msg["content"], str):
                target_text = last_msg["content"]
            elif isinstance(last_msg["content"], list):
                for item in last_msg["content"]:
                    if item.get("type") == "text": target_text += item.get("text", "")

        # OpenWebUI åå°ä»»åŠ¡æ£€æµ‹ï¼šé¿å…å¯¹è‡ªåŠ¨ç”Ÿæˆçš„ä»»åŠ¡è¿›è¡Œè·¯ç”± [1]
        if any(re.search(p, target_text, re.I) for p in [r"### Task", r"Suggest", r"Generate a concise"]):
            return "vibes_master", modified_messages

        # å¼ºåˆ¶è·¯ç”±é€»è¾‘
        if has_image and extracted_text:
            if any(x in extracted_text for x in ["âˆ«", "âˆ‘", "âˆš", "matrix", "\\frac", "theorem", "proof"]):
                logger.info("ğŸ“ [Force Route] Math detected -> deepthink")
                return "deepthink", modified_messages
            if any(x in extracted_text for x in ["def ", "class ", "import ", "console.log", "return ", "void "]):
                logger.info("ğŸ’» [Force Route] Code detected -> logic_king")
                return "logic_king", modified_messages

        # CrossEncoder è¯­ä¹‰æ‰“åˆ†
        ce_scores_raw = self.ce_model.predict([[target_text, v] for v in Config.ROUTING_DESCRIPTIONS.values()])
        ce_scores = {l: float(s) for l, s in zip(self.full_labels, ce_scores_raw)}
        
        sorted_scores = dict(sorted(ce_scores.items(), key=lambda item: item[1], reverse=True))
        logger.info(f"ğŸ“Š [Routing Scores] {json.dumps(sorted_scores, ensure_ascii=False)}")
        
        res = max(ce_scores, key=ce_scores.get)
        logger.info(f"ğŸš¦ Route Decision: {res}")
        return res, modified_messages

    async def route(self, messages: List[Dict]) -> Tuple[str, List[Dict]]:
        return await asyncio.get_event_loop().run_in_executor(self.executor, self._get_fused_decision, messages)

# ===================== FastAPI æœåŠ¡ =====================
brain: Optional[NekoBrain] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global brain
    brain = NekoBrain()
    yield

app = FastAPI(lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str 
    stream: Optional[bool] = True

@app.post("/v1/chat/completions")
async def chat(req: ChatRequest):
    try:
        label, processed_msgs = await brain.route(req.messages)
        
        # å®‰å…¨æ€§å›é€€ï¼šå¦‚æœè·¯ç”±ç»“æœåŒ…å«å›¾ç‰‡ä½†æ¨¡å‹ä¸æ”¯æŒï¼ˆégpt-5.1ï¼‰ï¼Œå¼ºåˆ¶å›é€€åˆ° gpt-5.1 [1]
        has_image_in_processed = any(
            isinstance(m.get("content"), list) and any(i.get("type") in ["image", "image_url"] for i in m["content"])
            for m in processed_msgs
        )
        if has_image_in_processed and label != "gpt-5.1":
            logger.warning(f"âš ï¸ Safety Fallback: Image found in {label} route. Redirecting to gpt-5.1.")
            label = "gpt-5.1"
            processed_msgs = req.messages

        target_model = Config.MODEL_MAP.get(label, "gemini-2.5-pro") if req.model in ["auto-router-1", "auto-router-2"] else req.model
        
        extra_kwargs = {}
        if target_model == "gpt-5.1":
            extra_kwargs["reasoning_effort"] = "high" 
            logger.info("ğŸ§  [GPT-5.1] Enforcing Reasoning Effort: HIGH")

        # æ£€æµ‹åå°ä»»åŠ¡
        msg_str_check = str(processed_msgs)
        is_background_task = any(p in msg_str_check for p in ["### Task", "Suggest", "Generate a concise"])
        
        # æ³¨å…¥æ ¼å¼æç¤º (ä»…é’ˆå¯¹é€»è¾‘ç±»æ¨¡å‹)
        if not is_background_task and label in ["logic_king", "deepthink"] and target_model != "gpt-5.1":
            processed_msgs = brain.inject_assistant_prompt(processed_msgs)

        logger.info(f"ğŸš€ Forwarding to: {target_model}")

        resp = await litellm.acompletion(
            model=target_model,
            messages=processed_msgs,
            stream=req.stream,
            api_base=Config.AGGREGATOR_BASE_URL,
            api_key=Config.AGGREGATOR_API_KEY,
            custom_llm_provider="openai",
            **extra_kwargs
        )

        if req.stream:
            async def gen():
                # --- ä¼ªè£…é€»è¾‘ ---
                display_model = target_model
                if display_model == "gpt-4o":
                    display_model = "gpt-5.1"

                # éåå°ä»»åŠ¡æ˜¾ç¤ºå‰ç¼€
                if not is_background_task:
                    prefix = f"> ğŸ˜¼ **NekoBrain**\n> Target: `{display_model}`\n> Label: `{label}`\n\n"
                    yield f"data: {json.dumps({'choices': [{'delta': {'content': prefix}, 'index': 0}], 'model': display_model})}\n\n"
                
                async for chunk in resp: 
                    yield f"data: {chunk.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
            return StreamingResponse(gen(), media_type="text/event-stream")
        return resp
    except Exception as e:
        logger.error(traceback.format_exc())
        return JSONResponse(status_code=500, content={"error": str(e)})

if __name__ == "__main__":
    import uvicorn
    torch.cuda.empty_cache()
    uvicorn.run(app, host="0.0.0.0", port=2000)
