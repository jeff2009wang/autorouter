import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import logging
import json
import traceback
import asyncio
import torch
import numpy as np
import time
import hashlib
from functools import lru_cache
from collections import OrderedDict
from torch.cuda import amp
from typing import List, Dict, Optional, Tuple
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor

from modelscope import snapshot_download
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
from sentence_transformers import CrossEncoder

from fastapi import FastAPI
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import litellm

# ===================== é…ç½®ä¸­å¿ƒ =====================
logging.basicConfig(
    level=logging.WARNING, 
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger("NekoBrain")

if os.getenv("NEKOBRAIN_DEBUG", "false").lower() == "true":
    logger.setLevel(logging.DEBUG)
else:
    logger.setLevel(logging.INFO)

# æŠ‘åˆ¶ litellm çš„å†—ä½™è¾“å‡º
logging.getLogger("litellm").setLevel(logging.WARNING)
logging.getLogger("LiteLLM").setLevel(logging.WARNING)
litellm.suppress_debug_info = True

# 1. æœ¬åœ°è·¯ç”±å¤§è„‘æ¨¡å‹
LOCAL_ROUTER_ID = "Qwen/Qwen2.5-7B-Instruct"

# 2. åœ¨çº¿è§†è§‰æ¨¡å‹é…ç½®
ONLINE_VLM_ID = "Qwen/Qwen3-VL-235B-A22B-Instruct"

# èšåˆAPIé…ç½®
AGGREGATOR_API_KEY = "sk-DuctN11czck6s758299ZoeipAjKmlhXcfhGchCZwQttQqI1o"
AGGREGATOR_BASE_URL = "http://192.168.50.165:3000/v1"

MODEL_MAP = {
    "flash_smart": "gemini-3-flash-preview",
    "pro_advanced": "gemini-3-pro",
    "code_technical": "gpt-5-codex-high",
    "code_architect": "claude-4-opus",
    "logic_reasoning": "gemini-3-pro-deepthink",
    "expert_xhigh": "gpt-5.2-xhigh"
}

# ===================== è·¯ç”±å¤§è„‘ =====================
class LRUCache:
    """ç®€å•çš„LRUç¼“å­˜å®ç°ï¼Œç”¨äºè·¯ç”±ç»“æœç¼“å­˜"""
    def __init__(self, max_size: int = 256):
        self.cache = OrderedDict()
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Tuple[str, List[Dict]]]:
        if key in self.cache:
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def put(self, key: str, value: Tuple[str, List[Dict]]):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)
    
    def clear(self):
        self.cache.clear()

class NekoBrain:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # ä¼˜åŒ–ï¼š2060 12GBæ˜¾å­˜æœ‰é™ï¼Œå‡å°‘å¹¶å‘çº¿ç¨‹æ•°
        self.executor = ThreadPoolExecutor(max_workers=4) 
        self.enable_perf_logging = True
        
        # æ·»åŠ è·¯ç”±ç»“æœç¼“å­˜ï¼ˆ256æ¡ï¼Œçº¦å ç”¨10-20MBå†…å­˜ï¼‰
        self.route_cache = LRUCache(max_size=256)
        
        self.full_labels = list(MODEL_MAP.keys())
        
        # å¿«é€Ÿè·¯å¾„å…³é”®è¯æ˜ å°„ï¼ˆæé«˜å‡†ç¡®åº¦å’Œé€Ÿåº¦ï¼‰
        self.quick_keywords = {
            "code_technical": ["def ", "class ", "import ", "function", "sql", "query", "python", "javascript", "java", "c++", "ä»£ç ", "ç¼–ç¨‹", "debug"],
            "code_architect": ["architecture", "design pattern", "system design", "microservice", "æ¶æ„", "è®¾è®¡æ¨¡å¼"],
            "logic_reasoning": ["prove", "theorem", "calculate", "solve", "equation", "integral", "å¾®åˆ†", "ç§¯åˆ†", "è¯æ˜", "è®¡ç®—"],
            "pro_advanced": ["creative", "story", "poem", "creative writing", "åˆ›ä½œ", "æ•…äº‹", "è¯—æ­Œ", "analysis"],
            "flash_smart": ["hello", "hi", "thanks", "ä½ å¥½", "è°¢è°¢"],
            "expert_xhigh": ["research", "paper", "academic", "research", "ç ”ç©¶", "å­¦æœ¯"]
        }
        
        logger.info("ğŸ§  Initializing NekoBrain (Online VLM + Local Router)...")
        logger.info(f"ğŸ‘ï¸ Using Online VLM via Aggregator: {ONLINE_VLM_ID}")

        # --- åŠ è½½æœ¬åœ°è·¯ç”±æ¨¡å‹ (Router) ---
        try:
            logger.info("ğŸ§  Loading Local Router (Qwen2.5-7B-Instruct)...")
            router_dir = snapshot_download(LOCAL_ROUTER_ID)
            
            bnb_config_router = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            # ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼šé™åˆ¶æ˜¾å­˜åˆ†é…ï¼Œä¼˜å…ˆä½¿ç”¨CPUå¸è½½
            max_memory = {0: "10GB"} if self.device == "cuda" else None

            self.router_model = AutoModelForCausalLM.from_pretrained(
                router_dir,
                torch_dtype=torch.float16,
                quantization_config=bnb_config_router,
                device_map="auto",
                max_memory=max_memory,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            # å°è¯•ä½¿ç”¨torch.compileåŠ é€Ÿï¼ˆPyTorch 2.0+ï¼Œå¯é€‰ï¼‰
            try:
                if hasattr(torch, 'compile') and self.device == "cuda":
                    logger.info("âš¡ Using torch.compile for optimization...")
                    self.router_model = torch.compile(self.router_model, mode="reduce-overhead")
            except Exception as e:
                logger.warning(f"torch.compile not available or failed: {e}")
            self.router_tokenizer = AutoTokenizer.from_pretrained(router_dir, trust_remote_code=True)
            logger.info("âœ… Router model loaded successfully")
            
            self._warmup_models()
                
        except Exception as e:
            logger.error(f"Failed to load Router: {e}")
            raise e

        # --- è¾…åŠ©å‘é‡æ¨¡å‹ (CPU è¿è¡Œ) ---
        self.ce_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device="cpu")
        
        logger.info("âœ… NekoBrain initialization complete")

    def _warmup_models(self):
        logger.info("ğŸ”¥ Warming up models...")
        try:
            dummy_text = "Hello, this is a warmup test."
            self._get_router_scores(dummy_text)
            logger.info("âœ… Model warmup complete")
        except Exception as e:
            logger.warning(f"Warmup failed (non-critical): {e}")

    def inject_assistant_prompt(self, messages: List[Dict]) -> List[Dict]:
        new_msgs = [m.copy() for m in messages]
        injection = {
            "role": "assistant",
            "content": "I will provide a professional solution. For code, I will optimize it. For math, I use LaTeX.\n"
        }
        new_msgs.append(injection)
        return new_msgs

    def _online_vlm_inference(self, messages: List[Dict], prompt_text: str) -> str:
        try:
            logger.info(f"ğŸ‘ï¸ Sending image to online VLM ({ONLINE_VLM_ID}) for OCR...")
            
            target_msg = None
            for m in reversed(messages):
                if isinstance(m.get("content"), list):
                    for item in m["content"]:
                        if item.get("type") in ["image", "image_url"]:
                            target_msg = m
                            break
                if target_msg: break
            
            if not target_msg: return ""

            vlm_messages = [
                target_msg,
                {"role": "user", "content": prompt_text}
            ]

            response = litellm.completion(
                model=f"openai/{ONLINE_VLM_ID}", 
                messages=vlm_messages,
                api_base=AGGREGATOR_BASE_URL,
                api_key=AGGREGATOR_API_KEY,
                max_tokens=1024,
                temperature=0.1
            )
            
            result = response.choices[0].message.content
            logger.info("âœ… Online OCR complete.")
            return result
        except Exception as e:
            logger.error(f"Online VLM Error: {e}")
            return ""

    def _quick_keyword_match(self, text: str) -> Optional[str]:
        """å¿«é€Ÿå…³é”®è¯åŒ¹é…ï¼Œè¿”å›æœ€å¯èƒ½çš„æ ‡ç­¾ï¼ˆç”¨äºåŠ é€Ÿç®€å•åœºæ™¯ï¼‰"""
        text_lower = text.lower()
        scores = {}
        for label, keywords in self.quick_keywords.items():
            matches = sum(1 for kw in keywords if kw.lower() in text_lower)
            if matches > 0:
                scores[label] = matches
        
        if scores:
            best_label = max(scores, key=scores.get)
            # åªæœ‰åŒ¹é…åº¦è¶³å¤Ÿé«˜ï¼ˆ>=2ä¸ªå…³é”®è¯ï¼‰æ‰ä½¿ç”¨å¿«é€Ÿè·¯å¾„
            if scores[best_label] >= 2:
                return best_label
        return None
    
    def _normalize_scores(self, raw_scores: Dict[str, float]) -> Dict[str, float]:
        """å°†åŸå§‹åˆ†æ•°å½’ä¸€åŒ–"""
        scores = list(raw_scores.values())
        if not scores: return raw_scores
        
        min_score, max_score = min(scores), max(scores)
        
        # é¿å…é™¤ä»¥é›¶
        if max_score == min_score: 
            return {label: 5.0 for label in raw_scores.keys()}
        
        # ã€å…³é”®ä¿®å¤ã€‘è¿™é‡Œä¹‹å‰å†™æˆäº† kï¼Œå¯¼è‡´ UnboundLocalErrorï¼Œç°å·²ä¿®æ­£ä¸º label
        return {
            label: 1.0 + 9.0 * (score - min_score) / (max_score - min_score) 
            for label, score in raw_scores.items()
        }

    def _get_embedding_scores(self, text: str) -> Dict[str, float]:
        DESCRIPTIONS = {
            "flash_smart": "General assistance, daily chat, simple questions, greetings.",
            "pro_advanced": "Complex analysis, creative writing, nuanced language understanding.",
            "code_technical": "Writing code, Python/C++/Java, SQL queries, debugging scripts.",
            "code_architect": "System design, software architecture, explaining technical concepts.",
            "logic_reasoning": "Advanced mathematics, physics, logic puzzles, scientific reasoning.",
            "expert_xhigh": "Specialized professional research, high-context analysis."
        }
        ce_scores_raw = self.ce_model.predict([[text, v] for v in DESCRIPTIONS.values()])
        raw_scores = {l: float(s) for l, s in zip(self.full_labels, ce_scores_raw)}
        return self._normalize_scores(raw_scores)

    @torch.no_grad()
    def _get_router_scores(self, text: str) -> Dict[str, float]:
        start_time = time.time()
        try:
            # ä½¿ç”¨å®Œæ•´çš„contextä»¥ç¡®ä¿å‡†ç¡®åº¦
            context_segment = text[:800]
            
            # è¯¦ç»†å®Œæ•´çš„promptï¼Œç¡®ä¿æ¨¡å‹å……åˆ†ç†è§£æ¯ä¸ªç±»åˆ«çš„å«ä¹‰
            prompt = (
                "Rate the user input for EACH category below. You MUST rate ALL 6 categories.\n"
                "Score: 1 = Not relevant, 10 = Perfect match\n\n"
                "Categories:\n"
                "1. flash_smart: General chat, greetings, simple questions, daily conversation\n"
                "2. pro_advanced: Complex analysis, creative writing, nuanced language understanding, detailed explanations\n"
                "3. code_technical: Programming, debugging, SQL queries, writing code in Python/C++/Java, technical scripts\n"
                "4. code_architect: System design, software architecture, explaining technical concepts, architectural patterns\n"
                "5. logic_reasoning: Math proofs, physics problems, logic puzzles, step-by-step reasoning, calculus, theorems\n"
                "6. expert_xhigh: Professional research, academic papers, high-context analysis, specialized knowledge\n\n"
                f"User Input: \"{context_segment}\"\n\n"
                "Output ALL 6 ratings in format: label:X (one per line, where X is a number from 1 to 10)."
            )
            messages = [{"role": "system", "content": "You are a precise classifier. Rate each category from 1 to 10 based on relevance."}, {"role": "user", "content": prompt}]
            
            text_input = self.router_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = self.router_tokenizer([text_input], return_tensors="pt").to(self.router_model.device)
            
            # ä¼˜åŒ–ï¼šå‡å°‘max_new_tokensï¼ˆä»120é™åˆ°80ï¼‰ï¼Œä½¿ç”¨KV cacheï¼Œä¼˜åŒ–ç”Ÿæˆé€Ÿåº¦
            with amp.autocast():
                generated_ids = self.router_model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=80,  # å‡å°‘ç”Ÿæˆtokenæ•°ï¼ŒåŠ é€Ÿæ¨ç†
                    temperature=0.1,
                    do_sample=False,
                    num_beams=1,
                    pad_token_id=self.router_tokenizer.eos_token_id,
                    use_cache=True  # å¯ç”¨KV cache
                )
            
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            response = self.router_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # ä¸ä½¿ç”¨æ­£åˆ™åŒ¹é…ï¼Œæ”¹ç”¨å­—ç¬¦ä¸²åˆ†å‰²å’Œè§£æï¼Œæé«˜å‡†ç¡®ç‡
            scores = {}
            for line in response.strip().split('\n'):
                line = line.strip()
                if ':' not in line:
                    continue
                    
                # å°è¯•å¤šç§æ ¼å¼ï¼šlabel:score, label: score, label=scoreç­‰
                for separator in [':', '=', ' ']:
                    if separator in line:
                        parts = line.split(separator, 1)
                        if len(parts) == 2:
                            potential_label = parts[0].strip().lower()
                            potential_score = parts[1].strip()
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥æ ‡ç­¾
                            for label in self.full_labels:
                                if label.lower() in potential_label or potential_label in label.lower():
                                    # å°è¯•æå–æ•°å­—åˆ†æ•°ï¼ˆä¸ä½¿ç”¨æ­£åˆ™ï¼‰
                                    score_str = ""
                                    for char in potential_score:
                                        if char.isdigit() or char == '.':
                                            score_str += char
                                        elif char in [' ', '\t'] and score_str:
                                            break
                                        elif char not in [' ', '\t'] and not (char.isdigit() or char == '.'):
                                            if score_str:
                                                break
                                    
                                    if score_str:
                                        try:
                                            score = float(score_str)
                                            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
                                            if 0 <= score <= 10:
                                                scores[label] = score
                                                break
                                        except ValueError:
                                            continue
            
            for label in self.full_labels:
                if label not in scores: scores[label] = 1.0
            
            if self.enable_perf_logging:
                logger.info(f"âš¡ Router: {(time.time() - start_time)*1000:.1f}ms")
            
            return scores
        except Exception as e:
            logger.error(f"Router scoring error: {e}")
            return {label: 1.0 for label in self.full_labels}

    def _get_text_hash(self, text: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬çš„hashç”¨äºç¼“å­˜"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _get_fused_decision(self, messages: List[Dict]) -> tuple[str, List[Dict]]:
        decision_start = time.time()
        target_text = ""
        modified_messages = messages 
        
        has_image = any(
            isinstance(m.get("content"), list) and any(item.get("type") in ["image", "image_url"] for item in m["content"])
            for m in messages[-2:]
        )
        
        if has_image:
            logger.info("ğŸ“¸ Image detected. Starting Online OCR...")
            extracted_text = self._online_vlm_inference(messages, "Detailed transcription of this image.")
            target_text = extracted_text
            modified_messages = []
            for m in messages:
                new_m = m.copy()
                if isinstance(new_m.get("content"), list):
                    new_m["content"] = f"ã€System Note: Image Content (OCR):ã€‘\n{extracted_text}"
                modified_messages.append(new_m)
        else:
            last_msg = messages[-1]
            if isinstance(last_msg["content"], str):
                target_text = last_msg["content"]
            elif isinstance(last_msg["content"], list):
                for item in last_msg["content"]:
                    if item.get("type") == "text": target_text += item.get("text", "")
        
        # ä¼˜åŒ–ï¼šæ£€æŸ¥ç¼“å­˜ï¼ˆè·³è¿‡å›¾ç‰‡åœºæ™¯ï¼Œå› ä¸ºOCRç»“æœå¯èƒ½ä¸åŒï¼‰
        if not has_image and target_text:
            text_hash = self._get_text_hash(target_text)
            cached_result = self.route_cache.get(text_hash)
            if cached_result:
                logger.info(f"âš¡ Cache hit! Route: {cached_result[0]} ({((time.time() - decision_start)*1000):.1f}ms)")
                return cached_result
        
        # ä¼˜åŒ–ï¼šå¿«é€Ÿè·¯å¾„ - å¯¹ç®€å•åœºæ™¯ä½¿ç”¨å…³é”®è¯åŒ¹é…
        if target_text and len(target_text) < 500:
            quick_label = self._quick_keyword_match(target_text)
            if quick_label:
                logger.info(f"âš¡ Quick path: {quick_label} ({((time.time() - decision_start)*1000):.1f}ms)")
                result = (quick_label, modified_messages)
                if not has_image and target_text:
                    text_hash = self._get_text_hash(target_text)
                    self.route_cache.put(text_hash, result)
                return result

        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            embedding_future = executor.submit(self._get_embedding_scores, target_text)
            router_future = executor.submit(self._get_router_scores, target_text)
            embedding_scores = embedding_future.result()
            router_scores = router_future.result()
        
        # ä¼˜åŒ–ï¼šæ”¹è¿›è¯„åˆ†èåˆç®—æ³•ï¼ˆåŠ æƒå¹³å‡ï¼Œrouteræƒé‡æ›´é«˜å› ä¸ºæ›´å‡†ç¡®ï¼‰
        final_scores = {}
        for label in self.full_labels:
            emb_score = embedding_scores.get(label, 5.0)
            router_score = router_scores.get(label, 1.0)
            # Routeræƒé‡0.6ï¼ŒEmbeddingæƒé‡0.4ï¼ˆå¯ä»¥æ ¹æ®æ•ˆæœè°ƒæ•´ï¼‰
            final_scores[label] = 0.6 * router_score + 0.4 * emb_score
        
        best_label = max(final_scores, key=final_scores.get)
        
        # ç¼“å­˜ç»“æœï¼ˆå›¾ç‰‡åœºæ™¯ä¸ç¼“å­˜ï¼‰
        if not has_image and target_text:
            result = (best_label, modified_messages)
            self.route_cache.put(text_hash, result)
        
        # ã€å…³é”®æ¢å¤ã€‘æ¢å¤äº†æ‚¨éœ€è¦çš„è¯¦ç»†è¡¨æ ¼è¾“å‡ºé€»è¾‘
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("="*60)
            logger.debug(f"Input: {target_text[:200]}...")
            logger.debug("-"*60)
            logger.debug("Scoring Details:")
            for label in self.full_labels:
                logger.debug(
                    f"  {label:15} | Emb: {embedding_scores.get(label, 0):.2f} | "
                    f"Router: {router_scores.get(label, 0):.2f} | "
                    f"Final: {final_scores[label]:.2f}"
                )
            logger.debug("-"*60)
            logger.debug(f"Final Decision: {best_label} ({(time.time() - decision_start)*1000:.1f}ms)")
            logger.debug("="*60)
        else:
            logger.info(f"ğŸ¯ Route: {best_label} ({(time.time() - decision_start)*1000:.1f}ms)")
        
        return best_label, modified_messages

    async def route(self, messages: List[Dict]) -> tuple[str, List[Dict]]:
        return await asyncio.get_event_loop().run_in_executor(self.executor, self._get_fused_decision, messages)

# ===================== FastAPI æœåŠ¡ =====================
brain: NekoBrain = None
@asynccontextmanager
async def lifespan(app: FastAPI):
    global brain
    brain = NekoBrain()
    yield

app = FastAPI(lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str 
    stream: Optional[bool] = True

@app.post("/v1/chat/completions")
async def chat(req: ChatRequest):
    try:
        # ç»Ÿä¸€å¤„ç†auto_routeræ¨¡å‹åç§°ï¼ˆauto_router1, auto_router2ç­‰éƒ½è§†ä¸ºauto_routerï¼‰
        if req.model and req.model.startswith("auto"):
            req.model = "auto_router"
        
        label, processed_msgs = await brain.route(req.messages)
        
        target_model = MODEL_MAP.get(label, "gemini-3-flash-preview")
        if "code" in label or "logic" in label:
            processed_msgs = brain.inject_assistant_prompt(processed_msgs)

        logger.info(f"ğŸš€ Routing to: {target_model}")

        resp = await litellm.acompletion(
            model=f"openai/{target_model}", 
            messages=processed_msgs,
            stream=req.stream,
            api_base=AGGREGATOR_BASE_URL,
            api_key=AGGREGATOR_API_KEY
        )

        if req.stream:
            async def gen():
                prefix = f"> ğŸ§  **NekoBrain**\n> Route: `{target_model}`\n"
                yield f"data: {json.dumps({'choices': [{'delta': {'content': prefix}, 'index': 0}], 'model': target_model})}\n\n"
                async for chunk in resp: 
                    yield f"data: {chunk.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
            return StreamingResponse(gen(), media_type="text/event-stream")
        return resp
    except Exception as e:
        logger.error(f"Request failed: {str(e)}")
        logger.error(traceback.format_exc())
        return JSONResponse(status_code=500, content={"error": str(e)})

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=2000)
